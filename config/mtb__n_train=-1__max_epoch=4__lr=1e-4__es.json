{
    "task": "mtb",
    "arch": "encoder_mtb__roberta_small",
    //"arch": "encoder_mtb__roberta_base",
    //"data_path": "../data/nki/bin_sample",
    "data_path": "../data/nki/bin-v3-threshold20-small",
    //"data_path": "../data/nki/bin-v3-threshold20",

    "mask_type": "start_end",
    "strong_prob": 0.8,
    "n_tries_neighbor": 100,
    "n_tries_text": 100,
    "alpha": 0.7,

    "entity_dim": 256,

    "encoder_layers": 12,
    "encoder_embed_dim": 256,
    //"encoder_embed_dim": 768,
    "encoder_ffn_embed_dim": 1024,
    //"encoder_ffn_embed_dim": 3072,
    "encoder_attention_heads": 4,
    //"encoder_attention_heads": 12,
    "encoder_output_layer_type": "entity_start",

    "max_epoch": 4,
    "warmup_updates": 2590, // 0.06 * max_epoch * num_updates 
    //"warmup_updates": 43413, // 0.06 * max_epoch * num_updates 
    "optimizer": "adam",
    "adam_betas": "(0.9, 0.98)",
    "adam_eps": 1e-6,
    "clip_norm": 0.0,
    "lr": [1e-4],
    "lr_scheduler": "polynomial_decay",

    "weight_decay": 0.1,
    "dropout": 0.1,
    "attention_dropout": 0.1,

    "max_sentences": 260, // max number of textA sentences per batch
    "max_tokens": 1.2e4, // max number of textA tokens per batch
    //"max_sentences": 32, // max number of textA sentences per batch
    //"max_tokens": 1e3, // max number of textA tokens per batch
    "max_positions": 128,
    "update_freq": [1],

    "required_batch_size_multiple": 1,
    "skip_invalid_size_inputs_valid_test": true,

    "n_train_examples": -1,
    "n_valid_examples": -1,
    "n_test_examples": -1,

    "num_workers": 3, // set at least 1 worker
    //"num_workers": 0, // set at least 1 worker
    "save_dir": "../save/",
    "restore_file": false,
    //"pretrain_encoder_path": "../data/roberta/roberta.base/model.pt",
    "pretrain_encoder_path": "../data/roberta/roberta.small/model.pt",
    "ddp_backend": "no_c10d",
    "no_epoch_checkpoints": false,
    "fp16": false,

    "criterion": "binary_cross_entropy_custom"
}

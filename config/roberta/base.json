{
    "encoder_layers": 12,
    "encoder_embed_dim": 768,
    "encoder_ffn_embed_dim": 3072,
    "encoder_attention_heads": 12,

    "weight_decay": 0.1,
    "dropout": 0.1,
    "attention_dropout": 0.1,

    "max_positions": 128,
    "pretrain_roberta_path": "../data/roberta/roberta.base/model.pt"
}

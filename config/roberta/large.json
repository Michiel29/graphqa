{
    "encoder_layers": 24,
    "encoder_embed_dim": 1024,
    "encoder_ffn_embed_dim": 4096,
    "encoder_attention_heads": 16,

    "weight_decay": 0.1,
    "dropout": 0.1,
    "attention_dropout": 0.1,

    "max_positions": 128,
    "pretrain_encoder_path": "../data/roberta/roberta.large/model.pt"
}

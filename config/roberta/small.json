{
    "encoder_layers": 12,
    "encoder_embed_dim": 256,
    "encoder_ffn_embed_dim": 1024,
    "encoder_attention_heads": 4,

    "weight_decay": 0.1,
    "dropout": 0.1,
    "attention_dropout": 0.1,

    "max_positions": 128,
    "pretrain_encoder_path": "../data/roberta/roberta.small/model.pt"
}

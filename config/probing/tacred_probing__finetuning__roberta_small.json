{
    "add_configs": ["../tacred/tacred.json", "../roberta/small.json", "../base.json"],

    "eval_downstream": false,

    "seed": 0,

    "arch": "encoder_gnn_eval__roberta_small",
    "gnn_layer_type": "mlp_concat",
    "gnn_layer_args": {
        "enc_dim": 256,
        "linear_dim": 256,
        "layer_sizes": [[256, 2]],
        "layer_norm": true,
        "n_gnn_layers": 1
    },
    "layer_sizes": [[256, 1], [1, 1]],
    "gnn_mlp_layer_norm": true,
    "entity_dim": 256,
    // "entity_dim": 512,
    // "entity_dim": 768,

    "task": "tacred_probing",
    "n_rules": 1000,
    "n_texts": 100,
    "n_chunks": 4,
    "mask_type": "start_end",
    "non_mask_rate": 1,
    // "non_mask_rate": 0,
    "encoder_output_layer_type": "entity_start_linear",

    "max_epoch": 10,
    "warmup_updates": 2553, // bsz=16, max_epoch=10
    // "warmup_updates": 1277, // bsz=32, max_epoch=10
    // "warmup_updates": 638, // bsz=64, max_epoch=10
    // "warmup_updates": 322, // bsz=128, max_epoch=10
    // "warmup_updates": 161, // bsz=256, max_epoch=10
    // "lr": [5e-6],
    // "lr": [1e-5],
    "lr": [2e-5],
    // "lr": [3e-5],
    // "lr_scheduler": "fixed",
    // "weight_decay": 0.01,

    // "max_sentences": 16,
    "max_sentences": 1,
    "update_freq": [1],
    // "update_freq": [2],
    // "update_freq": [4],
    // "update_freq": [8],
    // "update_freq": [16],
    "max_tokens": 2e4,

    "num_workers": 0,
    // "load_component_prefix": "encoder",

    // "restore_file": "/data1/aarchan/self_inference/save/task_tacred/arch_encoder_tacred__roberta_large/encoder_output_layer_type_entity_start_linear/entity_dim_256/lr_3e-05/max_sentences_16/update_freq_4/04m_01d_02h_42m/checkpoints/checkpoint_best.pt",

    "no_epoch_checkpoints": true

}

{
    "task": "fewrel",
    "data_path": "../data/fewrel/bin",
    "n_way": 5,
    "n_shot": 1,
    "dataset_size": 1000,

    "arch": "encoder_fewrel__roberta_base",
    "entity_dim": 256,

    "encoder_layers": 12,
    "encoder_embed_dim": 768,
    "encoder_ffn_embed_dim": 3072,
    "encoder_attention_heads": 12,
    "encoder_output_layer_type": "bag_of_words",
    //"encoder_output_layer_type": "head_tail_concat",

    "max_sentences": 10,
    "max_tokens": 2000,
    "required-batch-size-multiple": 1,

    "num_workers": 2,
    "save_dir": "../save/checkpoints",
    //"load_checkpoint": "../save/triplet_inference/encoder_triplet__roberta_small/lr_0.001/02m_14d_14h_41m/checkpoints/checkpoint_best.pt",
    "pretrain_encoder_path": "../data/roberta/roberta.base/model.pt",

    "ddp_backend": "no_c10d",
    "skip_invalid_size_inputs_valid_test": true,
    "max_positions": 128,

    "load_component_prefix": "encoder",

    "criterion": "cross_entropy_custom"

}
